## ğŸ”¥ å¿…é¡»èƒŒä¸‹æ¥çš„ä»£ç æ¨¡æ¿

### 1. å®šä¹‰æ¨¡å‹ï¼ˆæœ€é‡è¦ï¼ï¼‰
```python
class MyModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MyModel, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x
```

### 2. è®­ç»ƒå¾ªç¯ï¼ˆå¿…è€ƒï¼ï¼‰
```python
for epoch in range(num_epochs):
    for inputs, labels in dataloader:
        # å‰å‘ä¼ æ’­
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        
        # åå‘ä¼ æ’­ï¼ˆå¿…é¡»è®°ä½é¡ºåºï¼ï¼‰
        optimizer.zero_grad()  # 1. æ¸…ç©ºæ¢¯åº¦
        loss.backward()        # 2. è®¡ç®—æ¢¯åº¦
        optimizer.step()       # 3. æ›´æ–°å‚æ•°
```

### 3. CNNæ¨¡æ¿
```python
class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc = nn.Linear(32 * 16 * 16, 10)
    
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)  # å±•å¹³
        x = self.fc(x)
        return x
```
## ğŸ’¡ é¢è¯•æŠ€å·§

### é‡åˆ°ä¸ä¼šçš„é—®é¢˜
âŒ ä¸è¦è¯´ï¼š"æˆ‘å®Œå…¨ä¸ä¼š"
âœ… åº”è¯¥è¯´ï¼š"è¿™ä¸ªæˆ‘äº†è§£ä¸€äº›åŸºæœ¬æ¦‚å¿µï¼Œèƒ½ä¸èƒ½ç»™æˆ‘ä¸€äº›æç¤ºï¼Ÿ"

### å†™ä»£ç æ—¶
1. **è¾¹å†™è¾¹è¯´**ï¼šè®²è§£ä½ çš„æ€è·¯
2. **å†™æ³¨é‡Š**ï¼šå±•ç¤ºä½ çš„é€»è¾‘æ¸…æ™°
3. **æµ‹è¯•ä»£ç **ï¼šå†™å®Œåè·‘ä¸€ä¸‹ï¼Œç¡®ä¿æ²¡æœ‰æ˜æ˜¾é”™è¯¯
4. **æ‰¿è®¤é”™è¯¯**ï¼šå‘ç°bugä¸»åŠ¨è¯´å‡ºæ¥å¹¶æ”¹æ­£

### å›ç­”é—®é¢˜æ—¶
1. **å…ˆè¯´åŸç†**ï¼šå…ˆè§£é‡Šæ˜¯ä»€ä¹ˆ
2. **å†ä¸¾ä¾‹å­**ï¼šç”¨ç®€å•ä¾‹å­è¯´æ˜
3. **æœ€åè¯´åº”ç”¨**ï¼šä»€ä¹ˆåœºæ™¯ä¸‹ç”¨

ä¾‹å¦‚é—®"ä»€ä¹ˆæ˜¯Dropout"ï¼š
- **åŸç†**ï¼šDropoutæ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œè®­ç»ƒæ—¶éšæœºä¸¢å¼ƒä¸€äº›ç¥ç»å…ƒ
- **ä¾‹å­**ï¼šæ¯”å¦‚Dropout(0.5)ï¼Œæ¯ä¸ªç¥ç»å…ƒæœ‰50%çš„æ¦‚ç‡è¢«ä¸´æ—¶ä¸¢å¼ƒ
- **åº”ç”¨**ï¼šç”¨æ¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œç‰¹åˆ«æ˜¯åœ¨å…¨è¿æ¥å±‚

## âš ï¸ é¢è¯•ä¸­å¯èƒ½é‡åˆ°çš„é™·é˜±

### é™·é˜±1ï¼šå¿˜è®°optimizer.zero_grad()
```python
# âŒ é”™è¯¯
loss.backward()
optimizer.step()

# âœ… æ­£ç¡®
optimizer.zero_grad()  # å¿…é¡»å…ˆæ¸…ç©ºï¼
loss.backward()
optimizer.step()
```

### é™·é˜±2ï¼šäº¤å‰ç†µåå†åŠ Softmax
```python
# âŒ é”™è¯¯
output = F.softmax(model(x))
loss = nn.CrossEntropyLoss()(output, labels)  # CrossEntropyLosså·²åŒ…å«Softmax

# âœ… æ­£ç¡®
output = model(x)  # ç›´æ¥è¾“å‡ºlogits
loss = nn.CrossEntropyLoss()(output, labels)
```

### é™·é˜±3ï¼šè¯„ä¼°æ—¶å¿˜è®°model.eval()
```python
# âŒ é”™è¯¯
with torch.no_grad():
    output = model(x)  # Dropoutå’ŒBNä»åœ¨è®­ç»ƒæ¨¡å¼

# âœ… æ­£ç¡®
model.eval()
with torch.no_grad():
    output = model(x)
```
## ğŸ“š æ¨èåç»­å­¦ä¹ è·¯å¾„

é¢è¯•åç»§ç»­å­¦ä¹ ï¼š
1. å®Œæ•´å­¦ä¹ ã€Šé±¼ä¹¦ã€‹ï¼ˆæ·±åº¦å­¦ä¹ å…¥é—¨ï¼‰
2. è·Ÿç€PyTorchå®˜æ–¹æ•™ç¨‹åšé¡¹ç›®
3. åœ¨Kaggleä¸Šå‚åŠ æ¯”èµ›
4. é˜…è¯»ç»å…¸è®ºæ–‡ï¼ˆLeNet, AlexNet, ResNetï¼‰
5. è‡ªå·±å®ç°ä¸€ä¸ªå®Œæ•´é¡¹ç›®

è®°ä½ï¼š**ç°åœ¨åªæ˜¯å¼€å§‹ï¼Œè·¯è¿˜å¾ˆé•¿ï¼**

