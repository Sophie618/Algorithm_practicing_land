# 大模型面试经典算法实现 - PyTorch版本

涵盖Transformer、Attention机制、位置编码等核心组件  
**这些是大模型面试的必考内容！**

---

## 题目1：实现 Scaled Dot-Product Attention（注意力机制核心）

**公式**: `Attention(Q,K,V) = softmax(QK^T / √d_k)V`

### 代码实现

```python
def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    缩放点积注意力机制
    Args:
        Q: Query矩阵 [batch_size, num_heads, seq_len_q, d_k]
        K: Key矩阵 [batch_size, num_heads, seq_len_k, d_k]
        V: Value矩阵 [batch_size, num_heads, seq_len_v, d_v]
        mask: 掩码 [batch_size, 1, 1, seq_len_k] 或 [batch_size, 1, seq_len_q, seq_len_k]
    Returns:
        output: 注意力输出 [batch_size, num_heads, seq_len_q, d_v]
        attention_weights: 注意力权重 [batch_size, num_heads, seq_len_q, seq_len_k]
    """
    d_k = Q.size(-1)
    
    # 1. 计算注意力分数: Q * K^T
    scores = torch.matmul(Q, K.transpose(-2, -1))  # [batch, heads, seq_q, seq_k]
    
    # 2. 缩放（防止梯度消失）
    scores = scores / math.sqrt(d_k)
    
    # 3. 应用mask（可选，用于padding或causal mask）
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    
    # 4. Softmax归一化
    attention_weights = F.softmax(scores, dim=-1)  # [batch, heads, seq_q, seq_k]
    
    # 5. 加权求和
    output = torch.matmul(attention_weights, V)  # [batch, heads, seq_q, d_v]
    
    return output, attention_weights
```

### 测试结果

- 输入 Q shape: `[2, 8, 10, 64]`
- 输出 output shape: `[2, 8, 10, 64]`
- 注意力权重 shape: `[2, 8, 10, 10]`
- 注意力权重和为1？: ✓

---

## 题目2：实现 Multi-Head Attention（多头注意力机制）

**这是Transformer的核心组件！**

### 代码实现

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1):
        """
        多头注意力机制
        Args:
            d_model: 模型维度（通常是512或768）
            num_heads: 头的数量（通常是8或12）
            dropout: dropout比率
        """
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0, "d_model必须能被num_heads整除"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # 每个头的维度
        
        # 定义Q、K、V的线性变换
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        
        # 输出的线性变换
        self.W_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def split_heads(self, x):
        """
        将最后一维拆分成(num_heads, d_k)
        x: [batch_size, seq_len, d_model]
        return: [batch_size, num_heads, seq_len, d_k]
        """
        batch_size, seq_len, d_model = x.size()
        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
    
    def combine_heads(self, x):
        """
        将多头拼接回来
        x: [batch_size, num_heads, seq_len, d_k]
        return: [batch_size, seq_len, d_model]
        """
        batch_size, num_heads, seq_len, d_k = x.size()
        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)
    
    def forward(self, query, key, value, mask=None):
        """
        前向传播
        Args:
            query: [batch_size, seq_len_q, d_model]
            key: [batch_size, seq_len_k, d_model]
            value: [batch_size, seq_len_v, d_model]
            mask: [batch_size, 1, 1, seq_len_k]
        """
        batch_size = query.size(0)
        
        # 1. 线性变换并拆分成多头
        Q = self.split_heads(self.W_q(query))  # [batch, heads, seq_q, d_k]
        K = self.split_heads(self.W_k(key))    # [batch, heads, seq_k, d_k]
        V = self.split_heads(self.W_v(value))  # [batch, heads, seq_v, d_k]
        
        # 2. 计算注意力
        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)
        
        # 3. 拼接多头
        attn_output = self.combine_heads(attn_output)  # [batch, seq_q, d_model]
        
        # 4. 最后的线性变换
        output = self.W_o(attn_output)
        output = self.dropout(output)
        
        return output, attn_weights
```

### 测试结果

- 输入 shape: `[2, 10, 512]`
- MHA输出 shape: `[2, 10, 512]`
- 注意力权重 shape: `[2, 8, 10, 10]`
- 参数量: ~1,050,000

---

## 题目3：实现 Sinusoidal Position Encoding（正弦位置编码）

**因为Transformer没有循环结构，需要位置编码来表示顺序信息**

### 公式

- `PE(pos, 2i) = sin(pos / 10000^(2i/d_model))`
- `PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))`

### 代码实现

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        """
        位置编码
        PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
        PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
        """
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        # 创建位置编码矩阵 [max_len, d_model]
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        # 计算div_term
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                            (-math.log(10000.0) / d_model))
        
        # 偶数位置用sin，奇数位置用cos
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        
        # 注册为buffer（不会被当作模型参数训练）
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        """
        x: [batch_size, seq_len, d_model]
        """
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)
```

### 测试结果

- 输入 shape: `[2, 50, 512]`
- 加入位置编码后 shape: `[2, 50, 512]`
- 位置编码矩阵 shape: `[100, 512]`
- 提示：可以用 `plt.imshow(pe_matrix.T)` 可视化位置编码模式

---

## 题目4：实现 Position-wise Feed-Forward Network

### 公式

`FFN(x) = max(0, xW1 + b1)W2 + b2`

### 代码实现

```python
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        """
        前馈神经网络（两层全连接）
        FFN(x) = max(0, xW1 + b1)W2 + b2
        Args:
            d_model: 输入输出维度（512）
            d_ff: 中间层维度（通常是2048，即4倍d_model）
            dropout: dropout比率
        """
        super(FeedForward, self).__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        """
        x: [batch_size, seq_len, d_model]
        """
        x = self.linear1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        return x
```

### 测试结果

- 输入 shape: `[2, 10, 512]`
- FFN输出 shape: `[2, 10, 512]`
- 参数量: ~2,100,000

---

## 题目5：实现完整的 Transformer Encoder Block

### 代码实现

```python
class TransformerEncoderBlock(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        """
        Transformer编码器块
        包含：Multi-Head Attention + Add&Norm + FFN + Add&Norm
        """
        super(TransformerEncoderBlock, self).__init__()
        
        # Multi-Head Attention
        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        
        # Feed-Forward Network
        self.ffn = FeedForward(d_model, d_ff, dropout)
        
        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        # Dropout
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        """
        x: [batch_size, seq_len, d_model]
        mask: [batch_size, 1, 1, seq_len]
        """
        # 1. Multi-Head Attention + Residual + Norm
        attn_output, _ = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout1(attn_output))
        
        # 2. Feed-Forward + Residual + Norm
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout2(ffn_output))
        
        return x
```

### 测试结果

- 输入 shape: `[2, 10, 512]`
- Encoder Block输出 shape: `[2, 10, 512]`
- 参数量: ~3,150,000

---

## 题目6：实现 RoPE (Rotary Position Embedding)

**LLaMA、ChatGLM等模型使用的位置编码方式**

### 代码实现

```python
def apply_rotary_pos_emb(x, cos, sin):
    """
    应用旋转位置编码
    x: [batch_size, num_heads, seq_len, head_dim]
    cos, sin: [1, 1, seq_len, head_dim]
    """
    # 将x分成两半
    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]
    
    # 应用旋转
    # [cos * x1 - sin * x2, sin * x1 + cos * x2]
    rotated = torch.cat([
        x1 * cos - x2 * sin,
        x2 * cos + x1 * sin
    ], dim=-1)
    
    return rotated

class RotaryPositionEmbedding(nn.Module):
    def __init__(self, dim, max_seq_len=2048, base=10000):
        """
        旋转位置编码
        Args:
            dim: 头的维度（必须是偶数）
            max_seq_len: 最大序列长度
            base: 基数（默认10000）
        """
        super().__init__()
        assert dim % 2 == 0, "dim必须是偶数"
        
        # 计算频率
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer('inv_freq', inv_freq)
        
        # 预计算cos和sin值
        t = torch.arange(max_seq_len).float()
        freqs = torch.einsum('i,j->ij', t, inv_freq)  # [seq_len, dim/2]
        emb = torch.cat([freqs, freqs], dim=-1)  # [seq_len, dim]
        
        self.register_buffer('cos_cached', emb.cos()[None, None, :, :])
        self.register_buffer('sin_cached', emb.sin()[None, None, :, :])
    
    def forward(self, q, k):
        """
        q, k: [batch_size, num_heads, seq_len, head_dim]
        """
        seq_len = q.shape[2]
        cos = self.cos_cached[:, :, :seq_len, :]
        sin = self.sin_cached[:, :, :seq_len, :]
        
        q_rotated = apply_rotary_pos_emb(q, cos, sin)
        k_rotated = apply_rotary_pos_emb(k, cos, sin)
        
        return q_rotated, k_rotated
```

### 测试结果

- 原始 Q shape: `[2, 8, 10, 64]`
- 旋转后 Q shape: `[2, 8, 10, 64]`
- **RoPE的优势**：相对位置编码，外推性更好

---

## 题目7：实现 KV Cache（用于自回归生成加速）

**核心思想：缓存历史K和V，避免重复计算**

### 代码实现

```python
class KVCacheAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        # KV Cache（初始为None）
        self.cache_k = None
        self.cache_v = None
    
    def forward(self, x, use_cache=False):
        """
        x: [batch_size, seq_len, d_model]
        use_cache: 是否使用缓存（推理时为True）
        """
        batch_size, seq_len, _ = x.size()
        
        # 计算Q, K, V
        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        
        if use_cache:
            if self.cache_k is not None:
                # 拼接历史的K和V
                K = torch.cat([self.cache_k, K], dim=2)
                V = torch.cat([self.cache_v, V], dim=2)
            
            # 更新缓存
            self.cache_k = K
            self.cache_v = V
        
        # 计算注意力
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        attn_weights = F.softmax(scores, dim=-1)
        attn_output = torch.matmul(attn_weights, V)
        
        # 拼接多头
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)
        output = self.W_o(attn_output)
        
        return output
    
    def clear_cache(self):
        """清空缓存"""
        self.cache_k = None
        self.cache_v = None
```

### 测试结果

- 第1次生成，输入长度: 5, cache_k shape: `[1, 8, 5, 64]`
- 第2次生成，输入长度: 1, cache_k shape: `[1, 8, 6, 64]`
- 第3次生成，输入长度: 1, cache_k shape: `[1, 8, 7, 64]`
- **加速原理**：每次只计算新token的Q，复用历史K和V

---

## 题目8：实现 Top-K 和 Top-P (Nucleus) Sampling

**这是控制大模型生成多样性的关键技术**

### Top-K Sampling

```python
def top_k_sampling(logits, k=50, temperature=1.0):
    """
    Top-K采样：只保留概率最高的k个token
    Args:
        logits: [batch_size, vocab_size] 未归一化的分数
        k: 保留的top-k数量
        temperature: 温度参数（>1更随机，<1更确定）
    """
    # 1. 应用温度
    logits = logits / temperature
    
    # 2. 找到top-k的值
    top_k_logits, top_k_indices = torch.topk(logits, k, dim=-1)
    
    # 3. 将非top-k的位置设为-inf
    logits_filtered = torch.full_like(logits, float('-inf'))
    logits_filtered.scatter_(-1, top_k_indices, top_k_logits)
    
    # 4. Softmax并采样
    probs = F.softmax(logits_filtered, dim=-1)
    next_token = torch.multinomial(probs, num_samples=1)
    
    return next_token
```

### Top-P (Nucleus) Sampling

```python
def top_p_sampling(logits, p=0.9, temperature=1.0):
    """
    Top-P (Nucleus) 采样：保留累积概率达到p的最小token集合
    Args:
        logits: [batch_size, vocab_size]
        p: 累积概率阈值（通常0.9或0.95）
        temperature: 温度参数
    """
    # 1. 应用温度
    logits = logits / temperature
    
    # 2. 计算概率并排序
    probs = F.softmax(logits, dim=-1)
    sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)
    
    # 3. 计算累积概率
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
    
    # 4. 移除累积概率超过p的token
    sorted_indices_to_remove = cumulative_probs > p
    # 保留第一个超过阈值的token
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = 0
    
    # 5. 创建mask并过滤
    indices_to_remove = sorted_indices_to_remove.scatter(-1, sorted_indices, sorted_indices_to_remove)
    logits_filtered = logits.clone()
    logits_filtered[indices_to_remove] = float('-inf')
    
    # 6. 重新计算概率并采样
    probs = F.softmax(logits_filtered, dim=-1)
    next_token = torch.multinomial(probs, num_samples=1)
    
    return next_token
```

---

## 题目9：LayerNorm vs BatchNorm（面试高频考点）

### 手动实现 LayerNorm

```python
class LayerNormManual(nn.Module):
    def __init__(self, normalized_shape, eps=1e-5):
        """
        手动实现LayerNorm（理解原理）
        LayerNorm在特征维度上归一化
        """
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(normalized_shape))
        self.beta = nn.Parameter(torch.zeros(normalized_shape))
        self.eps = eps
    
    def forward(self, x):
        """
        x: [batch_size, seq_len, d_model]
        """
        # 在最后一个维度上计算均值和方差
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        
        # 归一化
        x_norm = (x - mean) / torch.sqrt(var + self.eps)
        
        # 缩放和平移
        out = self.gamma * x_norm + self.beta
        
        return out
```

### 关键区别

- **BatchNorm**: 在batch维度归一化，适合CNN
- **LayerNorm**: 在特征维度归一化，适合Transformer（不依赖batch）

---

## 题目10：实现 Causal Mask（自回归语言模型的关键）

**防止模型'看到未来'的信息**

### 代码实现

```python
def create_causal_mask(seq_len):
    """
    创建因果掩码（下三角矩阵）
    返回: [seq_len, seq_len]
    """
    mask = torch.tril(torch.ones(seq_len, seq_len))
    return mask

def create_padding_mask(seq, pad_idx=0):
    """
    创建padding掩码
    seq: [batch_size, seq_len]
    返回: [batch_size, 1, 1, seq_len]
    """
    mask = (seq != pad_idx).unsqueeze(1).unsqueeze(2)
    return mask
```

### 示例输出

Causal Mask (5x5):
```
1 0 0 0 0
1 1 0 0 0
1 1 1 0 0
1 1 1 1 0
1 1 1 1 1
```

**含义**：位置i只能看到位置≤i的信息（1表示可见，0表示屏蔽）

---

## 题目11：生成Attention权重用于可视化

### 示例

句子: "I love AI"

Attention权重矩阵 (行=query, 列=key):

|        | I    | love | AI   |
|--------|------|------|------|
| **I**    | 0.70 | 0.20 | 0.10 |
| **love** | 0.30 | 0.50 | 0.20 |
| **AI**   | 0.10 | 0.30 | 0.60 |

### 解读

- 'love'对'I'的权重是0.3，说明它关注主语
- 'AI'对'love'的权重是0.3，说明它关注动词
- 这就是Attention捕捉到的语义关系！

---

## 面试重点总结：大模型核心知识点

### 1. Scaled Dot-Product Attention

- **公式**：`Attention(Q,K,V) = softmax(QK^T/√d_k)V`
- **为什么除以√d_k？** 防止内积过大导致梯度消失
- **时间复杂度**：O(n²d)，n是序列长度

### 2. Multi-Head Attention

- 多头可以学习不同的表示子空间
- 并行计算多个attention，最后拼接
- **参数量**：4×d_model²（Q、K、V、O四个矩阵）

### 3. 位置编码

- **Sinusoidal PE**：固定的三角函数编码
- **Learned PE**：可学习的位置嵌入
- **RoPE**：旋转位置编码，相对位置，外推性好

### 4. Transformer架构

- **Encoder**: Self-Attention + FFN + LayerNorm + Residual
- **Decoder**: Masked Self-Attention + Cross-Attention + FFN
- **Pre-LN vs Post-LN**：训练稳定性不同

### 5. 推理优化

- **KV Cache**：缓存历史K和V，加速自回归生成
- **Flash Attention**：IO优化，减少显存读写
- **PagedAttention**：类似虚拟内存管理KV Cache

### 6. 采样策略

- **Greedy**：贪心选最大概率（确定但重复）
- **Top-K**：限制候选集大小
- **Top-P (Nucleus)**：动态候选集，累积概率阈值
- **Temperature**：控制分布的尖锐度

### 7. 常见面试问题

**Q: Transformer为什么比RNN好？**  
A: 并行化、长距离依赖、可解释性

**Q: Self-Attention vs Cross-Attention？**  
A: Self是Q=K=V，Cross是Q来自decoder，K/V来自encoder

**Q: LayerNorm vs BatchNorm？**  
A: LN在特征维度归一化，不依赖batch；BN在batch维度

**Q: 如何处理长文本？**  
A: 滑动窗口、稀疏注意力、RoPE外推、位置插值

### 8. 最新进展（加分项）

- **LLaMA**: RoPE + RMSNorm + SwiGLU
- **GPT-4**: MoE (Mixture of Experts)
- **FlashAttention-2**: 更快的attention实现
- **GQA (Grouped Query Attention)**: 减少KV Cache大小

---

## 建议

1. ✅ 手写MHA的forward过程（高频！）
2. ✅ 解释Attention机制的物理意义
3. ✅ 对比不同位置编码方案
4. ✅ 了解推理优化技术（KV Cache、量化等）
5. ✅ 关注最新模型架构（LLaMA、Mistral等）

---

**所有大模型面试算法实现完成！**

