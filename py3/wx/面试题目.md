# æ·±åº¦å­¦ä¹ é¢è¯•é¢˜ç›®

## ğŸ“š ç†è®ºé—®é¢˜ç­”æ¡ˆ

### 1. ä»€ä¹ˆæ˜¯Tensorï¼Ÿä¸NumPyæ•°ç»„çš„åŒºåˆ«ï¼Ÿ

- **Tensorï¼ˆå¼ é‡ï¼‰**ï¼šPyTorchä¸­çš„å¤šç»´æ•°ç»„ï¼Œæ˜¯æ·±åº¦å­¦ä¹ æ¡†æ¶çš„åŸºç¡€æ•°æ®ç»“æ„ï¼Œç±»ä¼¼äºNumPyçš„ndarray

**ä¸»è¦åŒºåˆ«**ï¼š
| ç‰¹æ€§ | Tensor | NumPy Array |
|------|--------|-------------|
| **GPUåŠ é€Ÿ** | âœ… æ”¯æŒ | âŒ ä¸æ”¯æŒï¼ˆä»…CPUï¼‰ |
| **è‡ªåŠ¨æ±‚å¯¼** | âœ… æ”¯æŒï¼ˆautogradï¼‰ | âŒ ä¸æ”¯æŒ |
| **æ·±åº¦å­¦ä¹ ** | âœ… åŸç”Ÿæ”¯æŒ | âŒ éœ€è¦å…¶ä»–æ¡†æ¶ |
| **é€Ÿåº¦** | GPUä¸Šæ›´å¿« | CPUä¸Šå¿« |
| **ç”¨é€”** | æ·±åº¦å­¦ä¹ ã€ç¥ç»ç½‘ç»œ | ç§‘å­¦è®¡ç®—ã€æ•°æ®åˆ†æ |

**ä»£ç ç¤ºä¾‹**ï¼š
```python
import torch
import numpy as np

# NumPyæ•°ç»„
np_array = np.array([1, 2, 3])

# PyTorch Tensor
tensor = torch.tensor([1, 2, 3])

# Tensorå¯ä»¥åœ¨GPUä¸Šè¿è¡Œ
tensor_gpu = tensor.cuda()  # ç§»åŠ¨åˆ°GPU

# Tensoræ”¯æŒè‡ªåŠ¨æ±‚å¯¼
x = torch.tensor([2.0], requires_grad=True)
y = x ** 2
y.backward()  # è‡ªåŠ¨è®¡ç®—æ¢¯åº¦
print(x.grad)  # è¾“å‡º: tensor([4.])
```


### 2. ä»€ä¹ˆæ˜¯åå‘ä¼ æ’­ï¼ŸPyTorchå¦‚ä½•å®ç°ï¼Ÿ



**åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰**ï¼š
- æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•
- ä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚åå‘ä¼ æ’­è¯¯å·®
- è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦
- ä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—å¯¼æ•°

**å·¥ä½œæµç¨‹**ï¼š
1. **å‰å‘ä¼ æ’­**ï¼šè¾“å…¥ â†’ éšè—å±‚ â†’ è¾“å‡º â†’ è®¡ç®—æŸå¤±
2. **åå‘ä¼ æ’­**ï¼šæŸå¤± â†’ è®¡ç®—æ¢¯åº¦ â†’ æ›´æ–°æƒé‡

**PyTorchå®ç°**ï¼š
```python
import torch
import torch.nn as nn

# 1. å®šä¹‰æ¨¡å‹
model = nn.Linear(10, 1)

# 2. å‰å‘ä¼ æ’­
x = torch.randn(5, 10)
y_true = torch.randn(5, 1)
y_pred = model(x)

# 3. è®¡ç®—æŸå¤±
loss = nn.MSELoss()(y_pred, y_true)

# 4. åå‘ä¼ æ’­ï¼ˆPyTorchè‡ªåŠ¨è®¡ç®—æ‰€æœ‰æ¢¯åº¦ï¼‰
loss.backward()  # â† è¿™ä¸€è¡Œå°±å®Œæˆäº†åå‘ä¼ æ’­ï¼

# 5. æŸ¥çœ‹æ¢¯åº¦
print(model.weight.grad)  # æƒé‡çš„æ¢¯åº¦
print(model.bias.grad)    # åç½®çš„æ¢¯åº¦
```

**PyTorchçš„è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶**ï¼š
- **è®¡ç®—å›¾ï¼ˆComputational Graphï¼‰**ï¼šè‡ªåŠ¨æ„å»ºæ“ä½œçš„ä¾èµ–å…³ç³»
- **åŠ¨æ€å›¾**ï¼šæ¯æ¬¡å‰å‘ä¼ æ’­éƒ½é‡æ–°æ„å»º
- **è‡ªåŠ¨å¾®åˆ†**ï¼šé€šè¿‡é“¾å¼æ³•åˆ™è‡ªåŠ¨è®¡ç®—æ¢¯åº¦

### 3. è¿‡æ‹Ÿåˆæ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•é˜²æ­¢ï¼Ÿ

**è¿‡æ‹Ÿåˆï¼ˆOverfittingï¼‰**ï¼š
- æ¨¡å‹åœ¨**è®­ç»ƒé›†ä¸Šè¡¨ç°å¾ˆå¥½**ï¼Œä½†åœ¨**æµ‹è¯•é›†/éªŒè¯é›†ä¸Šè¡¨ç°å¾ˆå·®**
- åŸå› ï¼šæ¨¡å‹"è®°ä½"äº†è®­ç»ƒæ•°æ®ï¼Œè€Œä¸æ˜¯å­¦åˆ°äº†é€šç”¨è§„å¾‹
- è¡¨ç°ï¼šè®­ç»ƒæŸå¤±â†“ï¼ŒéªŒè¯æŸå¤±â†‘

**é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ–¹æ³•**ï¼š

#### 1ï¸âƒ£ **Dropoutï¼ˆéšæœºå¤±æ´»ï¼‰**
```python
import torch.nn as nn

class ModelWithDropout(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(100, 50)
        self.dropout = nn.Dropout(0.5)  # 50%çš„ç¥ç»å…ƒè¢«éšæœºä¸¢å¼ƒ
        self.fc2 = nn.Linear(50, 10)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.dropout(x)  # è®­ç»ƒæ—¶éšæœºä¸¢å¼ƒï¼Œæ¨ç†æ—¶ä¸ä¸¢å¼ƒ
        x = self.fc2(x)
        return x
```

#### 2ï¸âƒ£ **æ­£åˆ™åŒ–ï¼ˆL1/L2ï¼‰**
```python
# L2æ­£åˆ™åŒ–ï¼ˆæƒé‡è¡°å‡ï¼‰
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)
```

#### 3ï¸âƒ£ **æ•°æ®å¢å¼º**
```python
from torchvision import transforms

transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),  # éšæœºç¿»è½¬
    transforms.RandomRotation(10),      # éšæœºæ—‹è½¬
    transforms.ColorJitter(0.2, 0.2)    # é¢œè‰²æŠ–åŠ¨
])
```

#### 4ï¸âƒ£ **Early Stoppingï¼ˆæ—©åœï¼‰**
```python
best_val_loss = float('inf')
patience = 5
counter = 0

for epoch in range(epochs):
    train_loss = train(model)
    val_loss = validate(model)
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        counter = 0
        torch.save(model.state_dict(), 'best_model.pth')
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping!")
            break
```

#### 5ï¸âƒ£ **å‡å°‘æ¨¡å‹å¤æ‚åº¦**
- å‡å°‘å±‚æ•°
- å‡å°‘ç¥ç»å…ƒæ•°é‡
- ä½¿ç”¨æ›´ç®€å•çš„æ¨¡å‹

#### 6ï¸âƒ£ **å¢åŠ è®­ç»ƒæ•°æ®**
- æ”¶é›†æ›´å¤šæ•°æ®
- æ•°æ®å¢å¼º


### 4. ä¼˜åŒ–å™¨SGDå’ŒAdamçš„åŒºåˆ«ï¼Ÿ

#### **SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰**

**ç‰¹ç‚¹**ï¼š
- æœ€åŸºç¡€çš„ä¼˜åŒ–å™¨
- æŒ‰å›ºå®šå­¦ä¹ ç‡æ›´æ–°å‚æ•°
- éœ€è¦æ‰‹åŠ¨è°ƒæ•´å­¦ä¹ ç‡

**å…¬å¼**ï¼š
```
Î¸ = Î¸ - lr * âˆ‡Î¸
```

**ä»£ç **ï¼š
```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
```

**ä¼˜ç‚¹**ï¼š
- ç®€å•ã€æ˜“ç†è§£
- æ”¶æ•›ç¨³å®šï¼ˆå¦‚æœå­¦ä¹ ç‡åˆé€‚ï¼‰

**ç¼ºç‚¹**ï¼š
- éœ€è¦æ‰‹åŠ¨è°ƒæ•´å­¦ä¹ ç‡
- æ”¶æ•›æ…¢
- å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜

---

#### **Adamï¼ˆAdaptive Moment Estimationï¼‰**

**ç‰¹ç‚¹**ï¼š
- è‡ªé€‚åº”å­¦ä¹ ç‡
- ç»“åˆäº†Momentumå’ŒRMSprop
- ä¸ºæ¯ä¸ªå‚æ•°ç»´æŠ¤ä¸åŒçš„å­¦ä¹ ç‡

**å…¬å¼**ï¼ˆç®€åŒ–ï¼‰ï¼š
```
m = Î²1 * m + (1 - Î²1) * âˆ‡Î¸     # ä¸€é˜¶çŸ©ä¼°è®¡ï¼ˆåŠ¨é‡ï¼‰
v = Î²2 * v + (1 - Î²2) * (âˆ‡Î¸)Â²  # äºŒé˜¶çŸ©ä¼°è®¡ï¼ˆRMSpropï¼‰
Î¸ = Î¸ - lr * m / (âˆšv + Îµ)
```

**ä»£ç **ï¼š
```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))
```

**ä¼˜ç‚¹**ï¼š
- è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡
- æ”¶æ•›å¿«
- é²æ£’æ€§å¼º
- é€‚ç”¨äºå¤§å¤šæ•°é—®é¢˜

**ç¼ºç‚¹**ï¼š
- å†…å­˜å ç”¨ç¨å¤§ï¼ˆéœ€è¦å­˜å‚¨må’Œvï¼‰
- æœ‰æ—¶æ³›åŒ–æ€§ä¸å¦‚SGD

---

#### **å¯¹æ¯”è¡¨æ ¼**

| ç‰¹æ€§ | SGD | Adam |
|------|-----|------|
| **å­¦ä¹ ç‡** | å›ºå®š | è‡ªé€‚åº” |
| **æ”¶æ•›é€Ÿåº¦** | æ…¢ | å¿« |
| **è°ƒå‚éš¾åº¦** | é«˜ | ä½ |
| **å†…å­˜å ç”¨** | å° | ç¨å¤§ |
| **é€‚ç”¨åœºæ™¯** | ç®€å•é—®é¢˜ã€éœ€è¦æ›´å¥½æ³›åŒ– | å¤§å¤šæ•°æ·±åº¦å­¦ä¹ ä»»åŠ¡ |
| **æ¨èåº¦** | â­â­â­ | â­â­â­â­â­ |

**é€‰æ‹©å»ºè®®**ï¼š
- **é»˜è®¤ç”¨Adam**ï¼šå¤§å¤šæ•°æƒ…å†µä¸‹è¡¨ç°å¥½
- **è¿½æ±‚æ³›åŒ–æ€§**ï¼šç”¨SGD + Momentum
- **å¤§æ¨¡å‹è®­ç»ƒ**ï¼šAdamWï¼ˆAdamçš„æ”¹è¿›ç‰ˆï¼‰

---

### 5. å·ç§¯å±‚çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ


å·ç§¯å±‚ï¼ˆConvolutional Layerï¼‰æ˜¯CNNçš„æ ¸å¿ƒï¼Œç”¨äºæå–å›¾åƒçš„å±€éƒ¨ç‰¹å¾ã€‚

#### **ä¸»è¦ä½œç”¨**ï¼š

1. **ç‰¹å¾æå–**
   - è¾¹ç¼˜æ£€æµ‹ï¼ˆä½å±‚ï¼‰
   - çº¹ç†è¯†åˆ«ï¼ˆä¸­å±‚ï¼‰
   - å½¢çŠ¶/å¯¹è±¡è¯†åˆ«ï¼ˆé«˜å±‚ï¼‰

2. **å‚æ•°å…±äº«**
   - åŒä¸€ä¸ªå·ç§¯æ ¸åœ¨æ•´ä¸ªå›¾åƒä¸Šæ»‘åŠ¨
   - å¤§å¤§å‡å°‘å‚æ•°é‡
   - ä¾‹å¦‚ï¼šå…¨è¿æ¥å±‚ 224Ã—224Ã—3 â†’ 1000 éœ€è¦ 150M å‚æ•°
   - å·ç§¯å±‚åªéœ€è¦å‡ åƒä¸ªå‚æ•°

3. **å¹³ç§»ä¸å˜æ€§**
   - ç‰¹å¾æ£€æµ‹ä¸ä½ç½®æ— å…³
   - æ— è®ºçŒ«åœ¨å›¾åƒå“ªé‡Œï¼Œéƒ½èƒ½è¯†åˆ«

4. **ä¿ç•™ç©ºé—´ä¿¡æ¯**
   - ç›¸é‚»åƒç´ çš„å…³ç³»è¢«ä¿ç•™
   - é€‚åˆå›¾åƒã€è§†é¢‘ç­‰ç©ºé—´æ•°æ®

#### **å·ç§¯æ“ä½œç¤ºä¾‹**ï¼š

```python
import torch
import torch.nn as nn

# å®šä¹‰å·ç§¯å±‚
conv = nn.Conv2d(
    in_channels=3,   # è¾“å…¥é€šé“ï¼ˆRGBï¼‰
    out_channels=64, # è¾“å‡ºé€šé“ï¼ˆç‰¹å¾å›¾æ•°é‡ï¼‰
    kernel_size=3,   # å·ç§¯æ ¸å¤§å° 3Ã—3
    stride=1,        # æ­¥é•¿
    padding=1        # å¡«å……
)

# è¾“å…¥å›¾åƒ [batch_size, channels, height, width]
x = torch.randn(1, 3, 224, 224)

# å·ç§¯æ“ä½œ
output = conv(x)
print(output.shape)  # [1, 64, 224, 224]
```

#### **å·ç§¯æ ¸çš„ä½œç”¨**ï¼š

```python
# è¾¹ç¼˜æ£€æµ‹å·ç§¯æ ¸ç¤ºä¾‹
edge_kernel = torch.tensor([
    [-1, -1, -1],
    [-1,  8, -1],
    [-1, -1, -1]
])
```

---

## ğŸ’» ç¼–ç¨‹é¢˜å®ç°

### é¢˜ç›®1ï¼šå®ç°ç®€å•çš„çº¿æ€§å›å½’ â­â­â­

**é—®é¢˜æè¿°**ï¼šç»™å®šæ•°æ® y = 2x + 3 + noiseï¼Œä½¿ç”¨PyTorchå®ç°çº¿æ€§å›å½’ï¼Œå­¦ä¹ å‡ºæƒé‡å’Œåç½®ã€‚

**å®Œæ•´ä»£ç **ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# ============= 1. ç”Ÿæˆæ•°æ® =============
# y = 2x + 3 + noise
torch.manual_seed(42)
x_train = torch.randn(100, 1)
y_train = 2 * x_train + 3 + torch.randn(100, 1) * 0.1

print("æ•°æ®ç¤ºä¾‹:")
print(f"x[:5] = {x_train[:5].squeeze()}")
print(f"y[:5] = {y_train[:5].squeeze()}")


# ============= 2. å®šä¹‰æ¨¡å‹ =============
class LinearRegression(nn.Module):
    def __init__(self):
        super(LinearRegression, self).__init__()
        self.linear = nn.Linear(1, 1)  # 1ä¸ªè¾“å…¥ï¼Œ1ä¸ªè¾“å‡º
    
    def forward(self, x):
        return self.linear(x)


# ============= 3. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ =============
model = LinearRegression()
criterion = nn.MSELoss()  # å‡æ–¹è¯¯å·®æŸå¤±
optimizer = optim.SGD(model.parameters(), lr=0.01)

print(f"\nåˆå§‹å‚æ•°:")
print(f"æƒé‡: {model.linear.weight.item():.4f}")
print(f"åç½®: {model.linear.bias.item():.4f}")


# ============= 4. è®­ç»ƒ =============
epochs = 100
losses = []

for epoch in range(epochs):
    # å‰å‘ä¼ æ’­
    outputs = model(x_train)
    loss = criterion(outputs, y_train)
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
    loss.backward()        # è®¡ç®—æ¢¯åº¦
    optimizer.step()       # æ›´æ–°å‚æ•°
    
    losses.append(loss.item())
    
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')


# ============= 5. æŸ¥çœ‹ç»“æœ =============
print(f'\næœ€ç»ˆå‚æ•°:')
print(f'æƒé‡: {model.linear.weight.item():.4f} (æœŸæœ›: 2.0)')
print(f'åç½®: {model.linear.bias.item():.4f} (æœŸæœ›: 3.0)')

# å¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰
# plt.figure(figsize=(12, 4))
# 
# # æŸå¤±æ›²çº¿
# plt.subplot(1, 2, 1)
# plt.plot(losses)
# plt.xlabel('Epoch')
# plt.ylabel('Loss')
# plt.title('Training Loss')
# 
# # æ‹Ÿåˆç»“æœ
# plt.subplot(1, 2, 2)
# plt.scatter(x_train.numpy(), y_train.numpy(), alpha=0.5, label='Data')
# x_line = torch.linspace(x_train.min(), x_train.max(), 100).reshape(-1, 1)
# y_line = model(x_line).detach().numpy()
# plt.plot(x_line.numpy(), y_line, 'r', label='Fitted line')
# plt.xlabel('x')
# plt.ylabel('y')
# plt.legend()
# plt.title('Linear Regression Result')
# plt.show()
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
Epoch [10/100], Loss: 0.0234
Epoch [20/100], Loss: 0.0089
Epoch [30/100], Loss: 0.0051
...
æœ€ç»ˆå‚æ•°:
æƒé‡: 2.0123 (æœŸæœ›: 2.0)
åç½®: 2.9876 (æœŸæœ›: 3.0)
```

---

### é¢˜ç›®2ï¼šå®ç°ä¸¤å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œ â­â­â­

**é—®é¢˜æè¿°**ï¼šå®ç°ä¸€ä¸ªä¸¤å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œè¿›è¡ŒäºŒåˆ†ç±»ã€‚

**å®Œæ•´ä»£ç **ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# ============= 1. ç”Ÿæˆæ•°æ® =============
# ç”ŸæˆäºŒåˆ†ç±»æ•°æ®
X, y = make_classification(
    n_samples=1000,      # æ ·æœ¬æ•°
    n_features=20,       # ç‰¹å¾æ•°
    n_classes=2,         # ç±»åˆ«æ•°
    n_informative=15,    # æœ‰ç”¨ç‰¹å¾æ•°
    random_state=42
)

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è½¬æ¢ä¸ºPyTorch Tensor
X_train = torch.FloatTensor(X_train)
y_train = torch.LongTensor(y_train)
X_test = torch.FloatTensor(X_test)
y_test = torch.LongTensor(y_test)

print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")


# ============= 2. å®šä¹‰ä¸¤å±‚ç¥ç»ç½‘ç»œ =============
class TwoLayerNet(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(TwoLayerNet, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)   # ç¬¬ä¸€å±‚
        self.relu = nn.ReLU()                         # æ¿€æ´»å‡½æ•°
        self.fc2 = nn.Linear(hidden_dim, output_dim)  # ç¬¬äºŒå±‚
    
    def forward(self, x):
        x = self.fc1(x)      # çº¿æ€§å˜æ¢
        x = self.relu(x)     # éçº¿æ€§æ¿€æ´»
        x = self.fc2(x)      # è¾“å‡ºå±‚
        return x


# ============= 3. åˆå§‹åŒ– =============
model = TwoLayerNet(input_dim=20, hidden_dim=64, output_dim=2)
criterion = nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±
optimizer = optim.Adam(model.parameters(), lr=0.001)

print(f"\næ¨¡å‹ç»“æ„:")
print(model)
print(f"å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")


# ============= 4. è®­ç»ƒ =============
epochs = 50
train_losses = []
test_accs = []

for epoch in range(epochs):
    # è®­ç»ƒæ¨¡å¼
    model.train()
    
    # å‰å‘ä¼ æ’­
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    train_losses.append(loss.item())
    
    # è¯„ä¼°ï¼ˆæ¯10ä¸ªepochï¼‰
    if (epoch + 1) % 10 == 0:
        model.eval()
        with torch.no_grad():
            # æµ‹è¯•é›†é¢„æµ‹
            test_outputs = model(X_test)
            pred = test_outputs.argmax(dim=1)
            acc = (pred == y_test).float().mean()
            test_accs.append(acc.item())
            
            # è®­ç»ƒé›†å‡†ç¡®ç‡
            train_pred = outputs.argmax(dim=1)
            train_acc = (train_pred == y_train).float().mean()
        
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, '
              f'Train Acc: {train_acc:.4f}, Test Acc: {acc:.4f}')


# ============= 5. æœ€ç»ˆè¯„ä¼° =============
model.eval()
with torch.no_grad():
    test_outputs = model(X_test)
    pred = test_outputs.argmax(dim=1)
    final_acc = (pred == y_test).float().mean()

print(f'\næœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_acc:.4f}')

# æ··æ·†çŸ©é˜µï¼ˆå¯é€‰ï¼‰
from sklearn.metrics import confusion_matrix, classification_report

print("\næ··æ·†çŸ©é˜µ:")
print(confusion_matrix(y_test.numpy(), pred.numpy()))
print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test.numpy(), pred.numpy()))
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
è®­ç»ƒé›†å¤§å°: torch.Size([800, 20])
æµ‹è¯•é›†å¤§å°: torch.Size([200, 20])

æ¨¡å‹ç»“æ„:
TwoLayerNet(
  (fc1): Linear(in_features=20, out_features=64, bias=True)
  (relu): ReLU()
  (fc2): Linear(in_features=64, out_features=2, bias=True)
)
å‚æ•°é‡: 1,410

Epoch [10/50], Loss: 0.3421, Train Acc: 0.8625, Test Acc: 0.8550
Epoch [20/50], Loss: 0.2134, Train Acc: 0.9100, Test Acc: 0.8850
...
æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: 0.9000
```

---

### é¢˜ç›®3ï¼šæ•°æ®é¢„å¤„ç†ï¼ˆæ ‡å‡†åŒ–ã€å½’ä¸€åŒ–ï¼‰â­â­â­

**é—®é¢˜æè¿°**ï¼šå®ç°æ•°æ®çš„æ ‡å‡†åŒ–å’Œå½’ä¸€åŒ–ã€‚

**å®Œæ•´ä»£ç **ï¼š

```python
import torch
import numpy as np

print("="*70)
print("æ•°æ®é¢„å¤„ç†ï¼šæ ‡å‡†åŒ– vs å½’ä¸€åŒ–")
print("="*70)


# ============= æ–¹æ³•1ï¼šæ ‡å‡†åŒ–ï¼ˆZ-score Normalizationï¼‰=============
def standardize(data):
    """
    æ ‡å‡†åŒ–ï¼š(x - mean) / std
    ç»“æœï¼šå‡å€¼0ï¼Œæ ‡å‡†å·®1
    """
    mean = data.mean(dim=0, keepdim=True)
    std = data.std(dim=0, keepdim=True)
    return (data - mean) / (std + 1e-8)  # åŠ 1e-8é˜²æ­¢é™¤0


print("\nã€æ–¹æ³•1ã€‘æ ‡å‡†åŒ–ï¼ˆStandardizationï¼‰")
print("-" * 70)

# ç”Ÿæˆæµ‹è¯•æ•°æ®
data = torch.randn(100, 5) * 10 + 50  # å‡å€¼50ï¼Œæ ‡å‡†å·®10å·¦å³
print(f"åŸå§‹æ•°æ® shape: {data.shape}")
print(f"åŸå§‹æ•°æ®å‡å€¼: {data.mean(dim=0)}")
print(f"åŸå§‹æ•°æ®æ ‡å‡†å·®: {data.std(dim=0)}")

# æ ‡å‡†åŒ–
data_standardized = standardize(data)
print(f"\næ ‡å‡†åŒ–åå‡å€¼: {data_standardized.mean(dim=0)}")
print(f"æ ‡å‡†åŒ–åæ ‡å‡†å·®: {data_standardized.std(dim=0)}")
print("â†’ å‡å€¼æ¥è¿‘0ï¼Œæ ‡å‡†å·®æ¥è¿‘1 âœ“")


# ============= æ–¹æ³•2ï¼šå½’ä¸€åŒ–ï¼ˆMin-Max Normalizationï¼‰=============
def normalize(data):
    """
    å½’ä¸€åŒ–ï¼š(x - min) / (max - min)
    ç»“æœï¼šç¼©æ”¾åˆ° [0, 1]
    """
    min_val = data.min(dim=0, keepdim=True)[0]
    max_val = data.max(dim=0, keepdim=True)[0]
    return (data - min_val) / (max_val - min_val + 1e-8)


print("\nã€æ–¹æ³•2ã€‘å½’ä¸€åŒ–ï¼ˆNormalizationï¼‰")
print("-" * 70)

data_normalized = normalize(data)
print(f"å½’ä¸€åŒ–åæœ€å°å€¼: {data_normalized.min(dim=0)[0]}")
print(f"å½’ä¸€åŒ–åæœ€å¤§å€¼: {data_normalized.max(dim=0)[0]}")
print("â†’ æ•°æ®åœ¨ [0, 1] èŒƒå›´å†… âœ“")


# ============= æ–¹æ³•3ï¼šå®Œæ•´çš„é¢„å¤„ç†ç±»ï¼ˆæ¨èï¼‰=============
print("\nã€æ–¹æ³•3ã€‘å®Œæ•´çš„æ•°æ®é¢„å¤„ç†ç±»")
print("-" * 70)

class DataPreprocessor:
    """æ•°æ®é¢„å¤„ç†å™¨ï¼ˆé€‚ç”¨äºè®­ç»ƒé›†/æµ‹è¯•é›†ï¼‰"""
    
    def __init__(self, method='standardize'):
        """
        Args:
            method: 'standardize' æˆ– 'normalize'
        """
        self.method = method
        self.mean = None
        self.std = None
        self.min = None
        self.max = None
    
    def fit(self, data):
        """åœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—ç»Ÿè®¡é‡"""
        if self.method == 'standardize':
            self.mean = data.mean(dim=0, keepdim=True)
            self.std = data.std(dim=0, keepdim=True)
        elif self.method == 'normalize':
            self.min = data.min(dim=0, keepdim=True)[0]
            self.max = data.max(dim=0, keepdim=True)[0]
        return self
    
    def transform(self, data):
        """åº”ç”¨é¢„å¤„ç†ï¼ˆè®­ç»ƒé›†/æµ‹è¯•é›†éƒ½ç”¨è¿™ä¸ªï¼‰"""
        if self.method == 'standardize':
            if self.mean is None or self.std is None:
                raise ValueError("è¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•ï¼")
            return (data - self.mean) / (self.std + 1e-8)
        
        elif self.method == 'normalize':
            if self.min is None or self.max is None:
                raise ValueError("è¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•ï¼")
            return (data - self.min) / (self.max - self.min + 1e-8)
    
    def fit_transform(self, data):
        """ä¸€æ­¥å®Œæˆ fit å’Œ transform"""
        self.fit(data)
        return self.transform(data)
    
    def inverse_transform(self, data):
        """åå‘è½¬æ¢ï¼ˆè¿˜åŸåŸå§‹æ•°æ®ï¼‰"""
        if self.method == 'standardize':
            return data * self.std + self.mean
        elif self.method == 'normalize':
            return data * (self.max - self.min) + self.min


# ä½¿ç”¨ç¤ºä¾‹
train_data = torch.randn(100, 5) * 10 + 50
test_data = torch.randn(20, 5) * 10 + 50

print("\nä½¿ç”¨æ ‡å‡†åŒ–é¢„å¤„ç†å™¨:")
preprocessor = DataPreprocessor(method='standardize')

# åœ¨è®­ç»ƒé›†ä¸Šfit
train_processed = preprocessor.fit_transform(train_data)
print(f"è®­ç»ƒé›†å‡å€¼: {train_processed.mean(dim=0)}")

# åœ¨æµ‹è¯•é›†ä¸Štransformï¼ˆä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡é‡ï¼ï¼‰
test_processed = preprocessor.transform(test_data)
print(f"æµ‹è¯•é›†å‡å€¼: {test_processed.mean(dim=0)}")
print("â†’ æµ‹è¯•é›†å‡å€¼å¯èƒ½ä¸æ˜¯0ï¼Œè¿™æ˜¯æ­£å¸¸çš„ï¼")


# ============= å®Œæ•´ç¤ºä¾‹ï¼šåœ¨ç¥ç»ç½‘ç»œè®­ç»ƒä¸­ä½¿ç”¨ =============
print("\nã€å®Œæ•´ç¤ºä¾‹ã€‘åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒä¸­ä½¿ç”¨é¢„å¤„ç†")
print("-" * 70)

# 1. ç”Ÿæˆæ•°æ®
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# è½¬æ¢ä¸ºTensor
X_train = torch.FloatTensor(X_train)
X_test = torch.FloatTensor(X_test)

# 2. é¢„å¤„ç†
preprocessor = DataPreprocessor(method='standardize')
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

print(f"è®­ç»ƒé›† shape: {X_train_processed.shape}")
print(f"æµ‹è¯•é›† shape: {X_test_processed.shape}")
print(f"è®­ç»ƒé›†å‡å€¼: {X_train_processed.mean():.6f}")
print(f"è®­ç»ƒé›†æ ‡å‡†å·®: {X_train_processed.std():.6f}")
comparison = """
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                â”‚   æ ‡å‡†åŒ–             â”‚   å½’ä¸€åŒ–            â”‚
â”‚                â”‚ (Standardization)   â”‚ (Normalization)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å…¬å¼           â”‚ (x - mean) / std   â”‚ (x - min)/(max-min)â”‚
â”‚ ç»“æœèŒƒå›´       â”‚ (-âˆ, +âˆ)           â”‚ [0, 1]             â”‚
â”‚ å‡å€¼           â”‚ 0                  â”‚ çº¦0.5              â”‚
â”‚ æ ‡å‡†å·®         â”‚ 1                  â”‚ ä¸å›ºå®š             â”‚
â”‚ é€‚ç”¨åœºæ™¯       â”‚ å¤§å¤šæ•°MLç®—æ³•       â”‚ ç¥ç»ç½‘ç»œ           â”‚
â”‚ å¯¹å¼‚å¸¸å€¼æ•æ„Ÿåº¦ â”‚ è¾ƒä½               â”‚ é«˜                 â”‚
â”‚ æ¨èåº¦         â”‚ â­â­â­â­â­          â”‚ â­â­â­             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é€‰æ‹©å»ºè®®:
  - é»˜è®¤ç”¨æ ‡å‡†åŒ–ï¼ˆé€‚ç”¨äºå¤§å¤šæ•°æƒ…å†µï¼‰
  - æ•°æ®å·²ç»åœ¨æœ‰é™èŒƒå›´å†… â†’ å½’ä¸€åŒ–
  - éœ€è¦ä¿æŒ0çš„ç‰¹æ®Šæ„ä¹‰ â†’ å½’ä¸€åŒ–
  - æœ‰å¼‚å¸¸å€¼ â†’ æ ‡å‡†åŒ–
"""

## ğŸ“ è®­ç»ƒå¾ªç¯æ¨¡æ¿ï¼ˆå¿…èƒŒï¼ï¼‰
```python
# ============= å®Œæ•´è®­ç»ƒå¾ªç¯æ¨¡æ¿ =============
for epoch in range(epochs):
    # ========== è®­ç»ƒé˜¶æ®µ ==========
    model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    train_loss = 0.0
    train_acc = 0.0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        # 1. æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
        data, target = data.to(device), target.to(device)
        
        # 2. å‰å‘ä¼ æ’­
        output = model(data)
        loss = criterion(output, target)
        
        # 3. åå‘ä¼ æ’­
        optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
        loss.backward()        # è®¡ç®—æ¢¯åº¦
        optimizer.step()       # æ›´æ–°å‚æ•°
        
        # 4. ç»Ÿè®¡
        train_loss += loss.item()
        pred = output.argmax(dim=1)
        train_acc += (pred == target).sum().item()
    
    # å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
    train_loss /= len(train_loader)
    train_acc /= len(train_loader.dataset)
    
    # ========== éªŒè¯é˜¶æ®µ ==========
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    val_loss = 0.0
    val_acc = 0.0
    
    with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
        for data, target in val_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = criterion(output, target)
            
            val_loss += loss.item()
            pred = output.argmax(dim=1)
            val_acc += (pred == target).sum().item()
    
    val_loss /= len(val_loader)
    val_acc /= len(val_loader.dataset)
    
    # æ‰“å°
    print(f'Epoch [{epoch+1}/{epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')
```


## ğŸ¯ å¿«é€Ÿè®°å¿†å¡ç‰‡

### **PyTorchæ ¸å¿ƒAPI**

```python
# æ¨¡å‹å®šä¹‰
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = nn.Linear(10, 5)
    
    def forward(self, x):
        return self.layer(x)

# æŸå¤±å‡½æ•°
criterion = nn.CrossEntropyLoss()  # åˆ†ç±»
criterion = nn.MSELoss()           # å›å½’

# ä¼˜åŒ–å™¨
optimizer = optim.Adam(model.parameters(), lr=0.001)
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

# è®­ç»ƒæ­¥éª¤
optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
loss.backward()        # åå‘ä¼ æ’­
optimizer.step()       # æ›´æ–°å‚æ•°

# æ¨¡å¼åˆ‡æ¢
model.train()  # è®­ç»ƒæ¨¡å¼
model.eval()   # è¯„ä¼°æ¨¡å¼

# æ¢¯åº¦æ§åˆ¶
with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
    output = model(x)
```

### **å¸¸ç”¨å±‚**

```python
# å…¨è¿æ¥å±‚
nn.Linear(in_features, out_features)

# å·ç§¯å±‚
nn.Conv2d(in_channels, out_channels, kernel_size)

# æ± åŒ–å±‚
nn.MaxPool2d(kernel_size)
nn.AvgPool2d(kernel_size)

# æ¿€æ´»å‡½æ•°
nn.ReLU()
nn.Sigmoid()
nn.Tanh()

# æ­£åˆ™åŒ–
nn.Dropout(p=0.5)
nn.BatchNorm2d(num_features)
```
## âœ… é¢è¯•å‡†å¤‡æ¸…å•

- [ ] èƒ½æ‰‹å†™çº¿æ€§å›å½’
- [ ] èƒ½æ‰‹å†™ä¸¤å±‚ç¥ç»ç½‘ç»œ
- [ ] èƒ½å®ç°æ•°æ®é¢„å¤„ç†
- [ ] èƒ½é»˜å†™è®­ç»ƒå¾ªç¯
- [ ] ç†è§£åå‘ä¼ æ’­åŸç†
- [ ] çŸ¥é“å¦‚ä½•é˜²æ­¢è¿‡æ‹Ÿåˆ
- [ ] ç†è§£SGDå’ŒAdamçš„åŒºåˆ«
- [ ] çŸ¥é“Tensorå’ŒNumPyçš„åŒºåˆ«
- [ ] ç†è§£å·ç§¯å±‚çš„ä½œç”¨

