# 深度学习面试题目

## 📚 理论问题答案

### 1. 什么是Tensor？与NumPy数组的区别？

- **Tensor（张量）**：PyTorch中的多维数组，是深度学习框架的基础数据结构，类似于NumPy的ndarray

**主要区别**：
| 特性 | Tensor | NumPy Array |
|------|--------|-------------|
| **GPU加速** | ✅ 支持 | ❌ 不支持（仅CPU） |
| **自动求导** | ✅ 支持（autograd） | ❌ 不支持 |
| **深度学习** | ✅ 原生支持 | ❌ 需要其他框架 |
| **速度** | GPU上更快 | CPU上快 |
| **用途** | 深度学习、神经网络 | 科学计算、数据分析 |

**代码示例**：
```python
import torch
import numpy as np

# NumPy数组
np_array = np.array([1, 2, 3])

# PyTorch Tensor
tensor = torch.tensor([1, 2, 3])

# Tensor可以在GPU上运行
tensor_gpu = tensor.cuda()  # 移动到GPU

# Tensor支持自动求导
x = torch.tensor([2.0], requires_grad=True)
y = x ** 2
y.backward()  # 自动计算梯度
print(x.grad)  # 输出: tensor([4.])
```


### 2. 什么是反向传播？PyTorch如何实现？



**反向传播（Backpropagation）**：
- 是训练神经网络的核心算法
- 从输出层向输入层反向传播误差
- 计算损失函数对每个参数的梯度
- 使用链式法则计算导数

**工作流程**：
1. **前向传播**：输入 → 隐藏层 → 输出 → 计算损失
2. **反向传播**：损失 → 计算梯度 → 更新权重

**PyTorch实现**：
```python
import torch
import torch.nn as nn

# 1. 定义模型
model = nn.Linear(10, 1)

# 2. 前向传播
x = torch.randn(5, 10)
y_true = torch.randn(5, 1)
y_pred = model(x)

# 3. 计算损失
loss = nn.MSELoss()(y_pred, y_true)

# 4. 反向传播（PyTorch自动计算所有梯度）
loss.backward()  # ← 这一行就完成了反向传播！

# 5. 查看梯度
print(model.weight.grad)  # 权重的梯度
print(model.bias.grad)    # 偏置的梯度
```

**PyTorch的自动求导机制**：
- **计算图（Computational Graph）**：自动构建操作的依赖关系
- **动态图**：每次前向传播都重新构建
- **自动微分**：通过链式法则自动计算梯度

### 3. 过拟合是什么？如何防止？

**过拟合（Overfitting）**：
- 模型在**训练集上表现很好**，但在**测试集/验证集上表现很差**
- 原因：模型"记住"了训练数据，而不是学到了通用规律
- 表现：训练损失↓，验证损失↑

**防止过拟合的方法**：

#### 1️⃣ **Dropout（随机失活）**
```python
import torch.nn as nn

class ModelWithDropout(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(100, 50)
        self.dropout = nn.Dropout(0.5)  # 50%的神经元被随机丢弃
        self.fc2 = nn.Linear(50, 10)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.dropout(x)  # 训练时随机丢弃，推理时不丢弃
        x = self.fc2(x)
        return x
```

#### 2️⃣ **正则化（L1/L2）**
```python
# L2正则化（权重衰减）
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)
```

#### 3️⃣ **数据增强**
```python
from torchvision import transforms

transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),  # 随机翻转
    transforms.RandomRotation(10),      # 随机旋转
    transforms.ColorJitter(0.2, 0.2)    # 颜色抖动
])
```

#### 4️⃣ **Early Stopping（早停）**
```python
best_val_loss = float('inf')
patience = 5
counter = 0

for epoch in range(epochs):
    train_loss = train(model)
    val_loss = validate(model)
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        counter = 0
        torch.save(model.state_dict(), 'best_model.pth')
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping!")
            break
```

#### 5️⃣ **减少模型复杂度**
- 减少层数
- 减少神经元数量
- 使用更简单的模型

#### 6️⃣ **增加训练数据**
- 收集更多数据
- 数据增强


### 4. 优化器SGD和Adam的区别？

#### **SGD（随机梯度下降）**

**特点**：
- 最基础的优化器
- 按固定学习率更新参数
- 需要手动调整学习率

**公式**：
```
θ = θ - lr * ∇θ
```

**代码**：
```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
```

**优点**：
- 简单、易理解
- 收敛稳定（如果学习率合适）

**缺点**：
- 需要手动调整学习率
- 收敛慢
- 容易陷入局部最优

---

#### **Adam（Adaptive Moment Estimation）**

**特点**：
- 自适应学习率
- 结合了Momentum和RMSprop
- 为每个参数维护不同的学习率

**公式**（简化）：
```
m = β1 * m + (1 - β1) * ∇θ     # 一阶矩估计（动量）
v = β2 * v + (1 - β2) * (∇θ)²  # 二阶矩估计（RMSprop）
θ = θ - lr * m / (√v + ε)
```

**代码**：
```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))
```

**优点**：
- 自动调整学习率
- 收敛快
- 鲁棒性强
- 适用于大多数问题

**缺点**：
- 内存占用稍大（需要存储m和v）
- 有时泛化性不如SGD

---

#### **对比表格**

| 特性 | SGD | Adam |
|------|-----|------|
| **学习率** | 固定 | 自适应 |
| **收敛速度** | 慢 | 快 |
| **调参难度** | 高 | 低 |
| **内存占用** | 小 | 稍大 |
| **适用场景** | 简单问题、需要更好泛化 | 大多数深度学习任务 |
| **推荐度** | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |

**选择建议**：
- **默认用Adam**：大多数情况下表现好
- **追求泛化性**：用SGD + Momentum
- **大模型训练**：AdamW（Adam的改进版）

---

### 5. 卷积层的作用是什么？


卷积层（Convolutional Layer）是CNN的核心，用于提取图像的局部特征。

#### **主要作用**：

1. **特征提取**
   - 边缘检测（低层）
   - 纹理识别（中层）
   - 形状/对象识别（高层）

2. **参数共享**
   - 同一个卷积核在整个图像上滑动
   - 大大减少参数量
   - 例如：全连接层 224×224×3 → 1000 需要 150M 参数
   - 卷积层只需要几千个参数

3. **平移不变性**
   - 特征检测与位置无关
   - 无论猫在图像哪里，都能识别

4. **保留空间信息**
   - 相邻像素的关系被保留
   - 适合图像、视频等空间数据

#### **卷积操作示例**：

```python
import torch
import torch.nn as nn

# 定义卷积层
conv = nn.Conv2d(
    in_channels=3,   # 输入通道（RGB）
    out_channels=64, # 输出通道（特征图数量）
    kernel_size=3,   # 卷积核大小 3×3
    stride=1,        # 步长
    padding=1        # 填充
)

# 输入图像 [batch_size, channels, height, width]
x = torch.randn(1, 3, 224, 224)

# 卷积操作
output = conv(x)
print(output.shape)  # [1, 64, 224, 224]
```

#### **卷积核的作用**：

```python
# 边缘检测卷积核示例
edge_kernel = torch.tensor([
    [-1, -1, -1],
    [-1,  8, -1],
    [-1, -1, -1]
])
```

---

## 💻 编程题实现

### 题目1：实现简单的线性回归 ⭐⭐⭐

**问题描述**：给定数据 y = 2x + 3 + noise，使用PyTorch实现线性回归，学习出权重和偏置。

**完整代码**：

```python
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# ============= 1. 生成数据 =============
# y = 2x + 3 + noise
torch.manual_seed(42)
x_train = torch.randn(100, 1)
y_train = 2 * x_train + 3 + torch.randn(100, 1) * 0.1

print("数据示例:")
print(f"x[:5] = {x_train[:5].squeeze()}")
print(f"y[:5] = {y_train[:5].squeeze()}")


# ============= 2. 定义模型 =============
class LinearRegression(nn.Module):
    def __init__(self):
        super(LinearRegression, self).__init__()
        self.linear = nn.Linear(1, 1)  # 1个输入，1个输出
    
    def forward(self, x):
        return self.linear(x)


# ============= 3. 初始化模型、损失函数、优化器 =============
model = LinearRegression()
criterion = nn.MSELoss()  # 均方误差损失
optimizer = optim.SGD(model.parameters(), lr=0.01)

print(f"\n初始参数:")
print(f"权重: {model.linear.weight.item():.4f}")
print(f"偏置: {model.linear.bias.item():.4f}")


# ============= 4. 训练 =============
epochs = 100
losses = []

for epoch in range(epochs):
    # 前向传播
    outputs = model(x_train)
    loss = criterion(outputs, y_train)
    
    # 反向传播
    optimizer.zero_grad()  # 清零梯度
    loss.backward()        # 计算梯度
    optimizer.step()       # 更新参数
    
    losses.append(loss.item())
    
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')


# ============= 5. 查看结果 =============
print(f'\n最终参数:')
print(f'权重: {model.linear.weight.item():.4f} (期望: 2.0)')
print(f'偏置: {model.linear.bias.item():.4f} (期望: 3.0)')

# 可视化（可选）
# plt.figure(figsize=(12, 4))
# 
# # 损失曲线
# plt.subplot(1, 2, 1)
# plt.plot(losses)
# plt.xlabel('Epoch')
# plt.ylabel('Loss')
# plt.title('Training Loss')
# 
# # 拟合结果
# plt.subplot(1, 2, 2)
# plt.scatter(x_train.numpy(), y_train.numpy(), alpha=0.5, label='Data')
# x_line = torch.linspace(x_train.min(), x_train.max(), 100).reshape(-1, 1)
# y_line = model(x_line).detach().numpy()
# plt.plot(x_line.numpy(), y_line, 'r', label='Fitted line')
# plt.xlabel('x')
# plt.ylabel('y')
# plt.legend()
# plt.title('Linear Regression Result')
# plt.show()
```

**输出示例**：
```
Epoch [10/100], Loss: 0.0234
Epoch [20/100], Loss: 0.0089
Epoch [30/100], Loss: 0.0051
...
最终参数:
权重: 2.0123 (期望: 2.0)
偏置: 2.9876 (期望: 3.0)
```

---

### 题目2：实现两层全连接神经网络 ⭐⭐⭐

**问题描述**：实现一个两层全连接神经网络进行二分类。

**完整代码**：

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# ============= 1. 生成数据 =============
# 生成二分类数据
X, y = make_classification(
    n_samples=1000,      # 样本数
    n_features=20,       # 特征数
    n_classes=2,         # 类别数
    n_informative=15,    # 有用特征数
    random_state=42
)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 转换为PyTorch Tensor
X_train = torch.FloatTensor(X_train)
y_train = torch.LongTensor(y_train)
X_test = torch.FloatTensor(X_test)
y_test = torch.LongTensor(y_test)

print(f"训练集大小: {X_train.shape}")
print(f"测试集大小: {X_test.shape}")


# ============= 2. 定义两层神经网络 =============
class TwoLayerNet(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(TwoLayerNet, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)   # 第一层
        self.relu = nn.ReLU()                         # 激活函数
        self.fc2 = nn.Linear(hidden_dim, output_dim)  # 第二层
    
    def forward(self, x):
        x = self.fc1(x)      # 线性变换
        x = self.relu(x)     # 非线性激活
        x = self.fc2(x)      # 输出层
        return x


# ============= 3. 初始化 =============
model = TwoLayerNet(input_dim=20, hidden_dim=64, output_dim=2)
criterion = nn.CrossEntropyLoss()  # 交叉熵损失
optimizer = optim.Adam(model.parameters(), lr=0.001)

print(f"\n模型结构:")
print(model)
print(f"参数量: {sum(p.numel() for p in model.parameters()):,}")


# ============= 4. 训练 =============
epochs = 50
train_losses = []
test_accs = []

for epoch in range(epochs):
    # 训练模式
    model.train()
    
    # 前向传播
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    
    # 反向传播
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    train_losses.append(loss.item())
    
    # 评估（每10个epoch）
    if (epoch + 1) % 10 == 0:
        model.eval()
        with torch.no_grad():
            # 测试集预测
            test_outputs = model(X_test)
            pred = test_outputs.argmax(dim=1)
            acc = (pred == y_test).float().mean()
            test_accs.append(acc.item())
            
            # 训练集准确率
            train_pred = outputs.argmax(dim=1)
            train_acc = (train_pred == y_train).float().mean()
        
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, '
              f'Train Acc: {train_acc:.4f}, Test Acc: {acc:.4f}')


# ============= 5. 最终评估 =============
model.eval()
with torch.no_grad():
    test_outputs = model(X_test)
    pred = test_outputs.argmax(dim=1)
    final_acc = (pred == y_test).float().mean()

print(f'\n最终测试准确率: {final_acc:.4f}')

# 混淆矩阵（可选）
from sklearn.metrics import confusion_matrix, classification_report

print("\n混淆矩阵:")
print(confusion_matrix(y_test.numpy(), pred.numpy()))
print("\n分类报告:")
print(classification_report(y_test.numpy(), pred.numpy()))
```

**输出示例**：
```
训练集大小: torch.Size([800, 20])
测试集大小: torch.Size([200, 20])

模型结构:
TwoLayerNet(
  (fc1): Linear(in_features=20, out_features=64, bias=True)
  (relu): ReLU()
  (fc2): Linear(in_features=64, out_features=2, bias=True)
)
参数量: 1,410

Epoch [10/50], Loss: 0.3421, Train Acc: 0.8625, Test Acc: 0.8550
Epoch [20/50], Loss: 0.2134, Train Acc: 0.9100, Test Acc: 0.8850
...
最终测试准确率: 0.9000
```

---

### 题目3：数据预处理（标准化、归一化）⭐⭐⭐

**问题描述**：实现数据的标准化和归一化。

**完整代码**：

```python
import torch
import numpy as np

print("="*70)
print("数据预处理：标准化 vs 归一化")
print("="*70)


# ============= 方法1：标准化（Z-score Normalization）=============
def standardize(data):
    """
    标准化：(x - mean) / std
    结果：均值0，标准差1
    """
    mean = data.mean(dim=0, keepdim=True)
    std = data.std(dim=0, keepdim=True)
    return (data - mean) / (std + 1e-8)  # 加1e-8防止除0


print("\n【方法1】标准化（Standardization）")
print("-" * 70)

# 生成测试数据
data = torch.randn(100, 5) * 10 + 50  # 均值50，标准差10左右
print(f"原始数据 shape: {data.shape}")
print(f"原始数据均值: {data.mean(dim=0)}")
print(f"原始数据标准差: {data.std(dim=0)}")

# 标准化
data_standardized = standardize(data)
print(f"\n标准化后均值: {data_standardized.mean(dim=0)}")
print(f"标准化后标准差: {data_standardized.std(dim=0)}")
print("→ 均值接近0，标准差接近1 ✓")


# ============= 方法2：归一化（Min-Max Normalization）=============
def normalize(data):
    """
    归一化：(x - min) / (max - min)
    结果：缩放到 [0, 1]
    """
    min_val = data.min(dim=0, keepdim=True)[0]
    max_val = data.max(dim=0, keepdim=True)[0]
    return (data - min_val) / (max_val - min_val + 1e-8)


print("\n【方法2】归一化（Normalization）")
print("-" * 70)

data_normalized = normalize(data)
print(f"归一化后最小值: {data_normalized.min(dim=0)[0]}")
print(f"归一化后最大值: {data_normalized.max(dim=0)[0]}")
print("→ 数据在 [0, 1] 范围内 ✓")


# ============= 方法3：完整的预处理类（推荐）=============
print("\n【方法3】完整的数据预处理类")
print("-" * 70)

class DataPreprocessor:
    """数据预处理器（适用于训练集/测试集）"""
    
    def __init__(self, method='standardize'):
        """
        Args:
            method: 'standardize' 或 'normalize'
        """
        self.method = method
        self.mean = None
        self.std = None
        self.min = None
        self.max = None
    
    def fit(self, data):
        """在训练集上计算统计量"""
        if self.method == 'standardize':
            self.mean = data.mean(dim=0, keepdim=True)
            self.std = data.std(dim=0, keepdim=True)
        elif self.method == 'normalize':
            self.min = data.min(dim=0, keepdim=True)[0]
            self.max = data.max(dim=0, keepdim=True)[0]
        return self
    
    def transform(self, data):
        """应用预处理（训练集/测试集都用这个）"""
        if self.method == 'standardize':
            if self.mean is None or self.std is None:
                raise ValueError("请先调用 fit() 方法！")
            return (data - self.mean) / (self.std + 1e-8)
        
        elif self.method == 'normalize':
            if self.min is None or self.max is None:
                raise ValueError("请先调用 fit() 方法！")
            return (data - self.min) / (self.max - self.min + 1e-8)
    
    def fit_transform(self, data):
        """一步完成 fit 和 transform"""
        self.fit(data)
        return self.transform(data)
    
    def inverse_transform(self, data):
        """反向转换（还原原始数据）"""
        if self.method == 'standardize':
            return data * self.std + self.mean
        elif self.method == 'normalize':
            return data * (self.max - self.min) + self.min


# 使用示例
train_data = torch.randn(100, 5) * 10 + 50
test_data = torch.randn(20, 5) * 10 + 50

print("\n使用标准化预处理器:")
preprocessor = DataPreprocessor(method='standardize')

# 在训练集上fit
train_processed = preprocessor.fit_transform(train_data)
print(f"训练集均值: {train_processed.mean(dim=0)}")

# 在测试集上transform（使用训练集的统计量！）
test_processed = preprocessor.transform(test_data)
print(f"测试集均值: {test_processed.mean(dim=0)}")
print("→ 测试集均值可能不是0，这是正常的！")


# ============= 完整示例：在神经网络训练中使用 =============
print("\n【完整示例】在神经网络训练中使用预处理")
print("-" * 70)

# 1. 生成数据
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 转换为Tensor
X_train = torch.FloatTensor(X_train)
X_test = torch.FloatTensor(X_test)

# 2. 预处理
preprocessor = DataPreprocessor(method='standardize')
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

print(f"训练集 shape: {X_train_processed.shape}")
print(f"测试集 shape: {X_test_processed.shape}")
print(f"训练集均值: {X_train_processed.mean():.6f}")
print(f"训练集标准差: {X_train_processed.std():.6f}")
comparison = """
┌────────────────┬────────────────────┬────────────────────┐
│                │   标准化             │   归一化            │
│                │ (Standardization)   │ (Normalization)    │
├────────────────┼────────────────────┼────────────────────┤
│ 公式           │ (x - mean) / std   │ (x - min)/(max-min)│
│ 结果范围       │ (-∞, +∞)           │ [0, 1]             │
│ 均值           │ 0                  │ 约0.5              │
│ 标准差         │ 1                  │ 不固定             │
│ 适用场景       │ 大多数ML算法       │ 神经网络           │
│ 对异常值敏感度 │ 较低               │ 高                 │
│ 推荐度         │ ⭐⭐⭐⭐⭐          │ ⭐⭐⭐             │
└────────────────┴────────────────────┴────────────────────┘

选择建议:
  - 默认用标准化（适用于大多数情况）
  - 数据已经在有限范围内 → 归一化
  - 需要保持0的特殊意义 → 归一化
  - 有异常值 → 标准化
"""

## 📝 训练循环模板（必背！）
```python
# ============= 完整训练循环模板 =============
for epoch in range(epochs):
    # ========== 训练阶段 ==========
    model.train()  # 设置为训练模式
    train_loss = 0.0
    train_acc = 0.0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        # 1. 数据移动到设备
        data, target = data.to(device), target.to(device)
        
        # 2. 前向传播
        output = model(data)
        loss = criterion(output, target)
        
        # 3. 反向传播
        optimizer.zero_grad()  # 清零梯度
        loss.backward()        # 计算梯度
        optimizer.step()       # 更新参数
        
        # 4. 统计
        train_loss += loss.item()
        pred = output.argmax(dim=1)
        train_acc += (pred == target).sum().item()
    
    # 平均损失和准确率
    train_loss /= len(train_loader)
    train_acc /= len(train_loader.dataset)
    
    # ========== 验证阶段 ==========
    model.eval()  # 设置为评估模式
    val_loss = 0.0
    val_acc = 0.0
    
    with torch.no_grad():  # 不计算梯度
        for data, target in val_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = criterion(output, target)
            
            val_loss += loss.item()
            pred = output.argmax(dim=1)
            val_acc += (pred == target).sum().item()
    
    val_loss /= len(val_loader)
    val_acc /= len(val_loader.dataset)
    
    # 打印
    print(f'Epoch [{epoch+1}/{epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')
```


## 🎯 快速记忆卡片

### **PyTorch核心API**

```python
# 模型定义
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = nn.Linear(10, 5)
    
    def forward(self, x):
        return self.layer(x)

# 损失函数
criterion = nn.CrossEntropyLoss()  # 分类
criterion = nn.MSELoss()           # 回归

# 优化器
optimizer = optim.Adam(model.parameters(), lr=0.001)
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

# 训练步骤
optimizer.zero_grad()  # 清零梯度
loss.backward()        # 反向传播
optimizer.step()       # 更新参数

# 模式切换
model.train()  # 训练模式
model.eval()   # 评估模式

# 梯度控制
with torch.no_grad():  # 不计算梯度
    output = model(x)
```

### **常用层**

```python
# 全连接层
nn.Linear(in_features, out_features)

# 卷积层
nn.Conv2d(in_channels, out_channels, kernel_size)

# 池化层
nn.MaxPool2d(kernel_size)
nn.AvgPool2d(kernel_size)

# 激活函数
nn.ReLU()
nn.Sigmoid()
nn.Tanh()

# 正则化
nn.Dropout(p=0.5)
nn.BatchNorm2d(num_features)
```
## ✅ 面试准备清单

- [ ] 能手写线性回归
- [ ] 能手写两层神经网络
- [ ] 能实现数据预处理
- [ ] 能默写训练循环
- [ ] 理解反向传播原理
- [ ] 知道如何防止过拟合
- [ ] 理解SGD和Adam的区别
- [ ] 知道Tensor和NumPy的区别
- [ ] 理解卷积层的作用

